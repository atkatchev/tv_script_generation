{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "In this project, you'll generate your own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using RNNs.  You'll be using part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "The data is already provided for you in `./data/Seinfeld_Scripts.txt` and you're encouraged to open that file and look at the text. \n",
    ">* As a first step, we'll load in this data and look at some samples. \n",
    "* Then, you'll be tasked with defining and training an RNN to generate a new script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_line_range` to view different parts of the data. This will give you a sense of the data you'll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #this function takes in a list of words in a text and \n",
    "    #it returns two dictionaries that map from our vocabulary \n",
    "    #to integer values and back. \n",
    "    #Creating a sorted vocabulary using the counter. \n",
    "    #list of words from most to least frequent according \n",
    "    #to the word counts returned by counter. \n",
    "    #Then integers are assigned in descending frequency order. \n",
    "    #So the most frequent word like B is given the integer 0, \n",
    "    #and the next most frequent is 1 and so on.\n",
    "\n",
    "    text_counts = Counter(text)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(text_counts, key=text_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    #print('int_to_vocab', int_to_vocab) #DEBUG\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    #print('vocab_to_int', vocab_to_int) #DEBUG\n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "        \n",
    "    return {\n",
    "        '.': '||Period||',\n",
    "        ',': '||Comma||',\n",
    "        '\"': '||Quotation_Mark||',\n",
    "        ';': '||Semicolon||',\n",
    "        '!': '||Exclamation_mark||',\n",
    "        '?': '||Question_mark||',\n",
    "        '(': '||Left_Parentheses||',\n",
    "        ')': '||Right_Parentheses||',\n",
    "        '-': '||Dash||',\n",
    "        \"\\n\": '||Return||'\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Let's start with the preprocessed input data. We'll use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, \n",
    "                                          batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    ">You can batch words using the DataLoader, but it will be up to you to create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`.\n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "5\n",
    "```\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #generates many batches of data.\n",
    "    #get the total number of complete batches that we can make in batches. \n",
    "    #first calculat how many characters were in a complete minibatch. \n",
    "    #So in one mini batch there's going to be\n",
    "    #batch size, time sequence, length, number of characters.\n",
    "    batch_size_total = batch_size * sequence_length\n",
    "    \n",
    "    #Then the number of complete batches that we can make is just the length \n",
    "    #of the array divided by the total number of characters in a mini minibatch. \n",
    "    #This double slash is an integer division, which will just round down any \n",
    "    #decimal leftover from this division. \n",
    "    #And with that we have the number of completely full batches that we can make.\n",
    "    #n_batches = len(words)//batch_size\n",
    "    #words = words[:n_batches*batch_size]\n",
    "    n_batches = len(words)//batch_size_total\n",
    "    \n",
    "    #Then we get our array arr and we take all the characters in the array up \n",
    "    #to N  batches times this total character size for our mini batch. \n",
    "    #So here we're making sure that we're keeping only enough characters to make \n",
    "    #full batches, and we may lose some characters here, but in general you're \n",
    "    #going to have enough data that getting rid of a last unfill batch is not\n",
    "    #really going to matter.\n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size_total]\n",
    "    \n",
    "    # TODO: Implement function    \n",
    "    features, targets = [], []\n",
    "    \n",
    "    #create feature tensors and target tensors of the correct size and \n",
    "    #content for a given sequence_length\n",
    "    for idx in range(0, (len(words) - sequence_length) ):\n",
    "        features.append(words[idx : idx + sequence_length])\n",
    "        #print('feature: ',features)\n",
    "        targets.append(words[idx + sequence_length])   \n",
    "        #print('target: ',targets)\n",
    "    \n",
    "    #Create DataLoaders for our training tensor datasets:\n",
    "    #1. Use pytorch's tensordataset to wrap tensor data into a known format \n",
    "    #2. Create DataLoaders and batch our training, for test Tensor datasets. \n",
    "    \n",
    "    #Creating my training tensor datasets. To create my training data, \n",
    "    #I'm passing in the tensor version of my features and targets and torch.from_numpy \n",
    "    #just takes in numpy arrays and converts them into tensors. \n",
    "    #Then for each tensor dataset that I just created, I'm passing it into pytorch's data loader \n",
    "   \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(features)), torch.from_numpy(np.asarray(targets)))\n",
    "    data_loader = torch.utils.data.DataLoader(data, shuffle=False , batch_size = batch_size)\n",
    "    \n",
    "    return data_loader\n",
    "    #return data_set_loader\n",
    "\n",
    "# there is no test for this function, but you are encouraged to create\n",
    "# print statements and tests of your own\n",
    "\n",
    "#test_text = range(50) #DEBUG\n",
    "#t_loader = batch_data(test_text, sequence_length=5, batch_size=10) #DEBUG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your dataloader \n",
    "\n",
    "You'll have to modify this code to test a batching function, but it should look fairly similar.\n",
    "\n",
    "Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "Your code should return something like the following (likely in a different order, if you shuffled your data):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### Sizes\n",
    "Your sample_x should be of size `(batch_size, sequence_length)` or (10, 5) in this case and sample_y should just have one dimension: batch_size (10). \n",
    "\n",
    "### Values\n",
    "\n",
    "You should also notice that the targets, sample_y, are the *next* value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[  0,   1,   2,   3,   4],\n",
      "        [  1,   2,   3,   4,   5],\n",
      "        [  2,   3,   4,   5,   6],\n",
      "        [  3,   4,   5,   6,   7],\n",
      "        [  4,   5,   6,   7,   8],\n",
      "        [  5,   6,   7,   8,   9],\n",
      "        [  6,   7,   8,   9,  10],\n",
      "        [  7,   8,   9,  10,  11],\n",
      "        [  8,   9,  10,  11,  12],\n",
      "        [  9,  10,  11,  12,  13]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for an LSTM/GRU hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model should be the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with `lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)`\n",
    "2. You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:\n",
    "\n",
    "```\n",
    "# reshape into (batch_size, seq_length, output_size)\n",
    "output = output.view(batch_size, -1, self.output_size)\n",
    "# get last batch\n",
    "out = output[:, -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "#embedding layer, a recurrent layer, and a final, linear layer with a sigmoid applied; \n",
    "    #defined in the __init__ function, according to passed in parameters.\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\" \n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        \n",
    "        #First, embedding layer, which should take in the size of the \n",
    "        #vocabulary (our number of integer tokens) and produce an embedding of \n",
    "        #embedding_dim size. So, as this model trains, this is going to create \n",
    "        #an embedding lookup table that has as many rows as we have word integers,\n",
    "        #and as many columns as the embedding dimension\n",
    "        #embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #Then, an LSTM layer, which takes in inputs of embedding_dim size.\n",
    "        #So, it's accepting embeddings as inputs, and producing an output and\n",
    "        #hidden state of a hidden size. Also specifying a number of layers,\n",
    "        #and a dropout value, and finally, setting batch_first to True\n",
    "        #because DataLoaders are used to batch the data.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        #Then, the LSTM outputs are passed to a dropout layer and then a \n",
    "        #fully-connected, linear layer that will produce output_size number \n",
    "        #of outputs. \n",
    "        #dropout layer\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        #And finally, a sigmoid layer is defined to convert the \n",
    "        #output to a value between 0-1.\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    #forward function, which takes in an input nn_input and a hidden \n",
    "    #state, I am going to pass an input through these layers in sequence.\n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function \n",
    "        \n",
    "        #First, the batch_size of input nn_input, which will be used for shaping the data\n",
    "        #Then, passing nn_input through the embedding layer first, to get embeddings as output\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        #These embeddings are passed to the lstm layer, alongside a hidden state, \n",
    "        #and this returns an lstm_output and a new hidden state. Then \n",
    "        #to stack up the outputs of the LSTM to pass to the last linear layer.\n",
    "        # embeddings and lstm_out\n",
    "        nn_input = nn_input.long()\n",
    "        embed_output = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embed_output, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)        \n",
    "        \n",
    "        #Then passing the reshaped lstm_output to a dropout layer \n",
    "        #and the linear layer, which should return a specified number of outputs \n",
    "        #that will be passed to the sigmoid activation function.\n",
    "        # dropout and fully-connected layer\n",
    "        \n",
    "        #out = self.dropout(lstm_out)####\n",
    "        #out = self.fc(out) #########\n",
    "        \n",
    "         \n",
    "        #sig_out = self.fc(out)\n",
    "        sig_out = self.fc(lstm_out)  ### removed dropout\n",
    "        # sigmoid function\n",
    "        #sig_out = self.sig(out) #########\n",
    "        \n",
    "        #returning only the last of these sigmoid outputs for a batch of input data, \n",
    "        #shape the outputs into a shape that is batch_size first. \n",
    "        #getting the last bacth by called `sig_out[:, -1], \n",
    "        #and that’s going to give the batch of last labels.\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        #returning that output and the hidden state produced by the LSTM layer.\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    #The hidden and cell states of an LSTM are a tuple of values and each\n",
    "    #of these is size (n_layers by batch_size, by hidden_dim). \n",
    "    #initializing these hidden weights to all zeros, and moving to a gpu if available.\n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM, \n",
    "        #and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            #a hidden and, a cell state that are saved as a tuple, hidden.\n",
    "            #The shape of the hidden and cell state is defined first by the number \n",
    "            #of layers, our model, the batch size of our input, and then the hidden \n",
    "            #dimension that we specified in model creation. And in this function, \n",
    "            #we're initializing the hidden weights all to zero and moving them to GPU\n",
    "            #if it's available.\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. Recall that you can get this loss by computing it, as usual, and calling `loss.item()`.\n",
    "\n",
    "**If a GPU is available, you should move your data to that GPU device, here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "        \n",
    "    #backpropagation, zero out any accumulated grradients \n",
    "    #and pass in our input tensors to our model. We also pass \n",
    "    #in the latest hidden state here (net(inputs, h)) and \n",
    "    #this returns a final output and a new hidden state.    \n",
    "    \n",
    "    #we detach any past and hidden state from its history    \n",
    "    #Recall that the hidden state of an LSTM layer is a tuple, \n",
    "    #and so here we're getting the data as a Tuple.\n",
    "       \n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    #clear accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "        \n",
    "    #get the output from the model\n",
    "    prediction, hidden = rnn(inp, hidden)\n",
    "    #loss calculation and backprop\n",
    "            \n",
    "    #Calculate the loss by looking at the predicted output               \n",
    "    loss = criterion(prediction, target)\n",
    "    #Take a backwards step, then we clip gradients and update weights perform an optimization step.  \n",
    "    loss.backward()\n",
    "    \n",
    "    #This kind of LSTM model has one main problem. \n",
    "    #Gradients can explode and get really, really big.\n",
    "    #We can clip the gradients. We just set some clip threshold \n",
    "    #and then if the gradient is larger than that threshold, \n",
    "    #we set it to that clip threshold and in code we do this by j\n",
    "    #ust passing in the parameters and the value that we want to \n",
    "    #clip the gradients at. In this case, the value five.\n",
    "                           \n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    #clipping + optimization \n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    \n",
    "    #moving one step in the right direction, \n",
    "    #updating the weights of our network.\n",
    "    optimizer.step()\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden\n",
    "\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The training loop is implemented for you in the `train_decoder` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. You'll set this parameter along with other parameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length =  10 # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = .001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 256\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 2000\n",
    "\n",
    "#print('vocab_size', vocab_size)\n",
    "#print('output_size', output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train. \n",
    "> **You should aim for a loss less than 3.5.** \n",
    "\n",
    "You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "DELAY = INTERVAL = 4 * 60  # interval time in seconds\n",
    "MIN_DELAY = MIN_INTERVAL = 2 * 60\n",
    "KEEPALIVE_URL = \"https://nebula.udacity.com/api/v1/remote/keep-alive\"\n",
    "TOKEN_URL = \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\"\n",
    "TOKEN_HEADERS = {\"Metadata-Flavor\":\"Google\"}\n",
    "\n",
    "\n",
    "def _request_handler(headers):\n",
    "    def _handler(signum, frame):\n",
    "        requests.request(\"POST\", KEEPALIVE_URL, headers=headers)\n",
    "    return _handler\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def active_session(delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    from workspace_utils import active session\n",
    "\n",
    "    with active_session():\n",
    "        # do long-running work here\n",
    "    \"\"\"\n",
    "    token = requests.request(\"GET\", TOKEN_URL, headers=TOKEN_HEADERS).text\n",
    "    headers = {'Authorization': \"STAR \" + token}\n",
    "    delay = max(delay, MIN_DELAY)\n",
    "    interval = max(interval, MIN_INTERVAL)\n",
    "    original_handler = signal.getsignal(signal.SIGALRM)\n",
    "    try:\n",
    "        signal.signal(signal.SIGALRM, _request_handler(headers))\n",
    "        signal.setitimer(signal.ITIMER_REAL, delay, interval)\n",
    "        yield\n",
    "    finally:\n",
    "        signal.signal(signal.SIGALRM, original_handler)\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "\n",
    "\n",
    "def keep_awake(iterable, delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    from workspace_utils import keep_awake\n",
    "\n",
    "    for i in keep_awake(range(5)):\n",
    "        # do iteration with lots of work here\n",
    "    \"\"\"\n",
    "    with active_session(delay, interval): yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 4.9304132409095764\n",
      "\n",
      "Epoch:    1/10    Loss: 4.503665405154228\n",
      "\n",
      "Epoch:    1/10    Loss: 4.3687747141122815\n",
      "\n",
      "Epoch:    2/10    Loss: 4.124601575159148\n",
      "\n",
      "Epoch:    2/10    Loss: 3.9403717595338823\n",
      "\n",
      "Epoch:    2/10    Loss: 3.8982175043821337\n",
      "\n",
      "Epoch:    3/10    Loss: 3.8125579905856415\n",
      "\n",
      "Epoch:    3/10    Loss: 3.711456812977791\n",
      "\n",
      "Epoch:    3/10    Loss: 3.692099633693695\n",
      "\n",
      "Epoch:    4/10    Loss: 3.640902316985238\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5669717919826507\n",
      "\n",
      "Epoch:    4/10    Loss: 3.561549678683281\n",
      "\n",
      "Epoch:    5/10    Loss: 3.518501907395688\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4592853020429613\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4562812983989715\n",
      "\n",
      "Epoch:    6/10    Loss: 3.431078700690545\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3882167429924013\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3848298746347427\n",
      "\n",
      "Epoch:    7/10    Loss: 3.363972353024755\n",
      "\n",
      "Epoch:    7/10    Loss: 3.3278395879268645\n",
      "\n",
      "Epoch:    7/10    Loss: 3.3236441373825074\n",
      "\n",
      "Epoch:    8/10    Loss: 3.3110214215994445\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2805206884145734\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2759983100891112\n",
      "\n",
      "Epoch:    9/10    Loss: 3.266237406925641\n",
      "\n",
      "Epoch:    9/10    Loss: 3.240086967349052\n",
      "\n",
      "Epoch:    9/10    Loss: 3.224386686563492\n",
      "\n",
      "Epoch:   10/10    Loss: 3.225628114463752\n",
      "\n",
      "Epoch:   10/10    Loss: 3.2048191146850584\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1910863745212557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "#from workspace_utils import active_session\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "with active_session():\n",
    "    # training the model\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** (Write answer, here)\n",
    "\n",
    "The models hyper-parameters were selected based on the the class lectures which provided a point of reference for the possible starting values and intuitions of each of the parameters as well as the above specification of a loss less than 3.5. The lectures broke hyper parameters into two categories; The first is optimizer hyper parameters which are the variables related more to the optimization and training process than to the model itself, including the learning rate, the mini-batch size, and the number of training iterations or epochs. The second is model hyper parameters, which are the variables that are more involved in the structure of the model, including the number of layers and hidden units and model specific hyper parameters for architectures like RNNs\n",
    "\n",
    "Initially, no matter how many times the hyper-parameters were changed the loss would not go below 9.0 for 20 epochs... Then after the sigmoid activation function was removed from the forward function the activation loss started at 5.0. Second, no matter how many times the hyper-parameters were changed the model would not go below 3.7. Then after the dropout was removed from the init and forward functions the loss specification was almost immediately achieved at less than 20 epochs. So for me the primary lesson here has been more on model architecture rather than the hyper parameters.\n",
    "\n",
    "Experimenting with sequence length ranging from 5 to 20 resulted in the following conclusion. The lower the sequence length the faster the 3.5 loss specification is reached. However, the higher the sequence length the less precise the model turns out to be. \n",
    "\n",
    "Experimenting with batch size dimensions ranging from 2 to 256 resulted in the following conclusion. Anything above 128 did not converge to the specified loss. However, 8 worked. As pointed out by the lectures a larger mini-batch size allows computational boosts that utilizes matrix multiplication, in the training calculations. But that comes at the expense of needing more memory for the training process, and generally, more computational resources. In practice, small mini-batch sizes have more noise in their error calculations, and this noise is often helpful in preventing the training process from stopping at local minima on the error curve rather than the global minima that creates the best model. So while the computational boost incentivizes us to increase the mini-batch size, this practical algorithmic benefit incentivizes us to actually make it smaller.\n",
    "\n",
    "Experimenting with hidden dimensions ranging from 128 to 512 resulted in the following conclusion. Did not observe much difference. The lectures pointed out that for a neural network to learn to approximate a function or a prediction task, it needs to have enough \"capacity\" to learn the function. The more complex the function, the more learning capacity the model will need. The number and architecture of the hidden units is the main measure for a model's learning capacity. If we provide the model with too much capacity, however, it might tend to over-fit and just try to memorize the training set. If you find your model over-fitting your data, meaning that the training accuracy is much better than the validation accuracy, you might want to try to decrease the number of hidden units. You could also utilize regularization techniques like dropouts or L2 regularization. So, as far as the number of hidden units is concerned, the more, the better. A little larger than the ideal number is not a problem, but a much larger value can often lead to the model over-fitting. So, if your model is not training, add more hidden units and track validation error. Keep adding hidden units until the validation starts getting worse. Another heuristic involving the first hidden layer is that setting it to a number larger than the number of the inputs has been observed to be beneficial in a number of tests.\n",
    "\n",
    "Experimenting with learning rate ranging from .0001 to .1 resulted in the sticking with within the norm not train too fast (high learning rate) overshooting possible convergence or getting stuck in a local minimum (low learning rate) if the rate was too low. The lectures outlined that the learning rate is the multiplier we use to push the weight towards the right direction. Calculating the gradient tells us which direction to go to decrease the error. If we do the calculation correctly, the gradient will point out which direction to go, meaning whether we should increase or decrease the current value of the weight. The learning rate is the multiplier we use to push the weight towards the right direction. Now, if we had made a miraculously correct choice for our learning rate, then we'd land on the best weight after only one training step. If the learning rate we chose was smaller than the ideal rate, then Our model can continue to learn until it finds a good value for the weight. So each training step it'll take a step closer until it lands on that best weight at the end. If, however, the learning rates had been too little, then our training error would be decreasing, but very slowly. And we might go through hundreds or thousands of training steps without reaching the best value for our model. And it's clear that in cases like this what we need to do is to increase the learning rate. One other case is that if we chose a learning rate that is larger than the ideal learning rate, our updated value would overshoot the ideal weight value. And then on the next update it would overshoot it the other way. But it will keep getting closer, and it would probably converge to a reasonable value. Where this becomes problematic though is when we choose a learning rate that is much larger than the ideal rate, more than twice as much. So in this case, we will see the weights taking a large step that not only overshoots the ideal weight, but it actually gets farther and farther from the best error that we can get at every step. A contributor to this divergence is the gradient. The gradient does not only contribute a direction but also a value that corresponds to the slope of the line tangent to the curve at that point. The higher the point is on the curve, the more steep the slope is, and the larger the value of the gradient is. So this makes the problem of the large learning rate even worse. So if our training error is actually increasing, we might want to try to decrease the learning rate and see what happens.\n",
    "\n",
    "The selection of embedding dimensions and hidden dimensions at 256 was based on the course lectures. The lectures showed that experimental results reporting a paper titled how to generate a good word embedding show that the performance of some tasks improve the larger we make the embedding, at least until a size of 200. In other tests, however, only marginal improvements are realized beyond the size of 50.\n",
    "\n",
    "Experimenting with number of RNN layers ranging from 2-3 was also based on the course lectures. The lectures showed that in practice, it's often the case that a three-layer neural net will outperform a two-layer net, but going even deeper rarely helps much more. The exception to this is convolutional neural networks where the deeper they are, the better they perform. The lectures also pointed out that the results for character level language modeling show that a depth of at least two is shown to be beneficial. In some cases increasing it to 3 shows mixed results. With that said I decided to stick with 2.\n",
    "\n",
    "The number of epochs was selected based on successfully reaching the minimum loss above. The lectures suggested to choose the right number of iterations or number of epochs for the training step, the metric typically used is the validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, **you can pause here and come back to this code at another time**. You can resume your progress by running the next cell, which will load in our word:id dictionaries _and_ load in your saved model by name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "With the network trained and saved, you'll use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "It's time to generate the text. Set `gen_length` to the length of TV script you want to generate and set `prime_word` to one of the following to start the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "You can set the prime word to _any word_ in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: rude, and then, uh, i guess we should have seen you.\n",
      "\n",
      "hoyt: so, you were going on the phone.\n",
      "\n",
      "hoyt: oh, no. no one-\n",
      "\n",
      "stu: no.\n",
      "\n",
      "elaine: oh, no no.\n",
      "\n",
      "stu: yes, yes, i got it!\n",
      "\n",
      "estelle: i guess that was a little problem.\n",
      "\n",
      "chiles: oh, no. i don't think so. i was a misprint.\n",
      "\n",
      "george: oh, yeah.\n",
      "\n",
      "elaine: hey!\n",
      "\n",
      "estelle: hello.\n",
      "\n",
      "jerry: hey!\n",
      "\n",
      "george: i can't believe it, i can't believe that i am.\n",
      "\n",
      "hoyt: yes, i think i was a little effeminate in a hotel time for nbc?\n",
      "\n",
      "elaine: yeah, yeah, well, i'm sorry.\n",
      "\n",
      "george: oh, you know, we could be able to do something.\n",
      "\n",
      "hoyt: oh, i forgot to get a reverse mein.\n",
      "\n",
      "hoyt: and the judge is the lowest getaway, and i have to be going.\n",
      "\n",
      "hoyt: oh.\n",
      "\n",
      "george: hey, jerry, this is unbelievable.\n",
      "\n",
      "hoyt: oh, that's not true.\n",
      "\n",
      "hoyt: you know, i don't know what to do.\n",
      "\n",
      "jerry: so what is that?\n",
      "\n",
      "jerry: i know i was in this city.\n",
      "\n",
      "george: i was wondering if you can go to paris.\n",
      "\n",
      "chiles:(whistles) oh, that's a good day of honor, but i was a misprint, and the shark character gets up.\n",
      "\n",
      "hoyt: and then, uh, the whole time is the lowest getaway incident that occurred and ignored.\n",
      "\n",
      "elaine: oh, i can't do it!\n",
      "\n",
      "jerry: oh! you know, the mets score is standing in the plane, then, i was employed.\n",
      "\n",
      "hoyt: and what are you doing?\n",
      "\n",
      "hoyt: we had a new pee.\n",
      "\n",
      "bookman: the prosecution is standing on the phone.\n",
      "\n",
      "jerry: oh, i forgot to go on this whole thing, and\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your favorite scripts\n",
    "\n",
    "Once you have a script that you like (or find interesting), save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Not Perfect\n",
    "It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.\n",
    "\n",
    "### Example generated script\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. \n",
    "\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save another copy as an HTML file by clicking \"File\" -> \"Download as..\"->\"html\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission. Once you download these files, compress them into one zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
